{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/titanic/train.csv\n",
      "/kaggle/input/titanic/gender_submission.csv\n",
      "/kaggle/input/titanic/test.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "   PassengerId  Survived\n",
      "0          892         0\n",
      "1          893         1\n",
      "2          894         0\n",
      "3          895         0\n",
      "4          896         1\n"
     ]
    }
   ],
   "source": [
    "train_data=pd.read_csv('/kaggle/input/titanic/train.csv')\n",
    "print(train_data.head())\n",
    "gender_submission=pd.read_csv('/kaggle/input/titanic/gender_submission.csv')\n",
    "print(gender_submission.head())\n",
    "\n",
    "\n",
    "# profile = ProfileReport(train_data, title=\"Pandas Profiling Report\")\n",
    "# profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             PassengerId  Survived    Pclass       Age     SibSp     Parch  \\\n",
      "PassengerId     1.000000 -0.005007 -0.035144  0.036847 -0.057527 -0.001652   \n",
      "Survived       -0.005007  1.000000 -0.338481 -0.077221 -0.035322  0.081629   \n",
      "Pclass         -0.035144 -0.338481  1.000000 -0.369226  0.083081  0.018443   \n",
      "Age             0.036847 -0.077221 -0.369226  1.000000 -0.308247 -0.189119   \n",
      "SibSp          -0.057527 -0.035322  0.083081 -0.308247  1.000000  0.414838   \n",
      "Parch          -0.001652  0.081629  0.018443 -0.189119  0.414838  1.000000   \n",
      "Fare            0.012658  0.257307 -0.549500  0.096067  0.159651  0.216225   \n",
      "\n",
      "                 Fare  \n",
      "PassengerId  0.012658  \n",
      "Survived     0.257307  \n",
      "Pclass      -0.549500  \n",
      "Age          0.096067  \n",
      "SibSp        0.159651  \n",
      "Parch        0.216225  \n",
      "Fare         1.000000  \n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n",
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n",
      "     Pclass     Sex  SibSp  Parch Embarked   Age     Fare  Survived\n",
      "0         3    male      1      0        S  22.0   7.2500         0\n",
      "1         1  female      1      0        C  38.0  71.2833         1\n",
      "2         3  female      0      0        S  26.0   7.9250         1\n",
      "3         1  female      1      0        S  35.0  53.1000         1\n",
      "4         3    male      0      0        S  35.0   8.0500         0\n",
      "..      ...     ...    ...    ...      ...   ...      ...       ...\n",
      "886       2    male      0      0        S  27.0  13.0000         0\n",
      "887       1  female      0      0        S  19.0  30.0000         1\n",
      "888       3  female      1      2        S   NaN  23.4500         0\n",
      "889       1    male      0      0        C  26.0  30.0000         1\n",
      "890       3    male      0      0        Q  32.0   7.7500         0\n",
      "\n",
      "[891 rows x 8 columns]\n",
      "     Pclass  Sex  SibSp  Parch  Embarked Age_Binned  Family  Survived\n",
      "0         3    1      1      0         2          2       0         0\n",
      "1         1    0      1      0         0          3       0         1\n",
      "2         3    0      0      0         2          2       0         1\n",
      "3         1    0      1      0         2          3       0         1\n",
      "4         3    1      0      0         2          3       0         0\n",
      "..      ...  ...    ...    ...       ...        ...     ...       ...\n",
      "886       2    1      0      0         2          2       0         0\n",
      "887       1    0      0      0         2          1       0         1\n",
      "888       3    0      1      2         2          2       1         0\n",
      "889       1    1      0      0         0          2       0         1\n",
      "890       3    1      0      0         1          3       0         0\n",
      "\n",
      "[891 rows x 8 columns]\n",
      "[[ 0.30110999 -1.06647119  1.15874882 -0.27856799]\n",
      " [-1.56684286 -0.03951371 -0.41566394 -0.06524527]\n",
      " [ 0.30105391  1.7922138   1.60828435  0.12468898]\n",
      " ...\n",
      " [-1.26300923 -0.20898957 -0.37473032  0.66661166]\n",
      " [ 0.73818559 -0.3860277  -1.44384356 -0.3478333 ]\n",
      " [ 0.12145903  1.81981976  2.05652834 -0.51783251]]\n",
      "[0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1\n",
      " 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1\n",
      " 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1\n",
      " 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0\n",
      " 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0\n",
      " 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0\n",
      " 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
      " 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1\n",
      " 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 1\n",
      " 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0\n",
      " 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0\n",
      " 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 1 1 1 0 0 1 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0\n",
      " 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 0 0 0\n",
      " 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
      " 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0]\n",
      "DT\n",
      "Accuracy: 0.8\n",
      "Accuracy F1: 0.7916666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Accuracy: 0.8\n",
      "Accuracy F1: 0.7934727180010199\n",
      "XGboost\n",
      "Accuracy: 0.8444444444444444\n",
      "Accuracy F1: 0.837962962962963\n",
      "GradientBoosting\n",
      "Accuracy: 0.8111111111111111\n",
      "Accuracy F1: 0.804122391499168\n"
     ]
    }
   ],
   "source": [
    "print(train_data.corr())\n",
    "print(train_data.describe())\n",
    "\n",
    "\n",
    "print(train_data.columns)\n",
    "train_data=train_data[['Pclass','Sex','SibSp','Parch','Embarked','Age','Fare','Survived']]\n",
    "print(train_data)\n",
    "\n",
    "train_data['Age']=train_data['Age'].fillna(train_data['Age'].mean())\n",
    "train_data['Fare']=train_data['Fare'].fillna(train_data['Fare'].mean())\n",
    "# train_data=train_data.drop_duplicates()\n",
    "\n",
    "\n",
    "train_data['Family']=0\n",
    "\n",
    "\n",
    "for x in range(len(train_data)):\n",
    "    sibsp=int(train_data['Parch'].iloc[x])\n",
    "  \n",
    "    if sibsp >0:\n",
    "        train_data['Family'].iloc[x]=1\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "bins = [0,10,20,30,40,50,60,70,80,90,100]\n",
    "labels = [0,1,2,3,4,5,6,7,8,9]\n",
    "train_data['Age_Binned'] = pd.cut(train_data['Age'], bins=bins,labels=labels)\n",
    "bins = [0,32,64,128,256,512,1025]\n",
    "labels = [0,1,2,3,4,5]\n",
    "train_data['Fare_Binned'] = pd.cut(train_data['Fare'], bins=bins,labels=labels)\n",
    "\n",
    "#encoding gender column\n",
    "\n",
    "enc_embarked = preprocessing.LabelEncoder()\n",
    "train_data['Embarked']=enc_embarked.fit_transform(train_data['Embarked'].tolist())\n",
    "\n",
    "enc_sex= preprocessing.LabelEncoder()\n",
    "train_data['Sex']=enc_sex.fit_transform(train_data['Sex'].tolist())\n",
    "\n",
    "filename = 'enc_embarked.sav'\n",
    "pickle.dump(enc_embarked, open(filename, 'wb'))\n",
    "\n",
    "filename = 'enc_sex.sav'\n",
    "pickle.dump(enc_sex, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "train_data=train_data[['Pclass','Sex','SibSp','Parch','Embarked','Age_Binned','Family','Survived']]\n",
    "print(train_data)\n",
    "\n",
    "np_input_data=train_data.iloc[:,0:len(train_data.columns)-1].values\n",
    "\n",
    "scaler =  MinMaxScaler()\n",
    "np_input_data_scaled=scaler.fit_transform(np_input_data)\n",
    "\n",
    "filename = 'scaling.sav'\n",
    "pickle.dump(scaler, open(filename, 'wb'))\n",
    "\n",
    "poly = PolynomialFeatures(3)\n",
    "np_input_data_scaled=poly.fit_transform(np_input_data_scaled)\n",
    "\n",
    "filename = 'polynomial.sav'\n",
    "pickle.dump(poly, open(filename, 'wb'))\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "np_input_data_scaled=pca.fit_transform(np_input_data_scaled)\n",
    "\n",
    "filename = 'pca.sav'\n",
    "pickle.dump(pca, open(filename, 'wb'))\n",
    "\n",
    "np_output_data=train_data.iloc[:,len(train_data.columns)-1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np_input_data_scaled, np_output_data, test_size=0.1, random_state=0)\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print('DT')\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Accuracy F1:\",metrics.f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "\n",
    "filename = 'dt.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf= RandomForestClassifier()\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print('Random Forest')\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Accuracy F1:\",metrics.f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "filename = 'random_forest.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1.0, gamma=1.5, gpu_id=-1,\n",
    "              importance_type='gain', interaction_constraints='',\n",
    "              learning_rate=0.02, max_delta_step=0, max_depth=6,\n",
    "              min_child_weight=1, monotone_constraints='()',\n",
    "              n_estimators=400, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
    "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "              subsample=1.0, tree_method='exact', validate_parameters=1,\n",
    "              verbosity=None)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print('XGboost')\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Accuracy F1:\",metrics.f1_score(y_test, y_pred, average='macro'))\n",
    "filename = 'xgb.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier( loss='deviance',n_estimators=100,learning_rate=0.1)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print('GradientBoosting')\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Accuracy F1:\",metrics.f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "\n",
    "filename = 'gradien_boost.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Pclass                                          Name     Sex  \\\n",
      "0          892       3                              Kelly, Mr. James    male   \n",
      "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
      "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
      "3          895       3                              Wirz, Mr. Albert    male   \n",
      "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
      "\n",
      "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
      "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
      "1  47.0      1      0   363272   7.0000   NaN        S  \n",
      "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
      "3  27.0      0      0   315154   8.6625   NaN        S  \n",
      "4  22.0      1      1  3101298  12.2875   NaN        S  \n",
      "     Pclass  Sex  SibSp  Parch  Embarked Age_Binned  Family  Survived\n",
      "0         3    1      1      0         2          2       0         0\n",
      "1         1    0      1      0         0          3       0         1\n",
      "2         3    0      0      0         2          2       0         1\n",
      "3         1    0      1      0         2          3       0         1\n",
      "4         3    1      0      0         2          3       0         0\n",
      "..      ...  ...    ...    ...       ...        ...     ...       ...\n",
      "886       2    1      0      0         2          2       0         0\n",
      "887       1    0      0      0         2          1       0         1\n",
      "888       3    0      1      2         2          2       1         0\n",
      "889       1    1      0      0         0          2       0         1\n",
      "890       3    1      0      0         1          3       0         0\n",
      "\n",
      "[891 rows x 8 columns]\n",
      "     Pclass  Sex  SibSp  Parch  Embarked Age_Binned  Family  Survived\n",
      "0         3    1      0      0         1          3       0         0\n",
      "1         3    0      1      0         2          4       0         1\n",
      "2         2    1      0      0         1          6       0         0\n",
      "3         3    1      0      0         2          2       0         0\n",
      "4         3    0      1      1         2          2       1         1\n",
      "..      ...  ...    ...    ...       ...        ...     ...       ...\n",
      "413       3    1      0      0         2          3       0         0\n",
      "414       1    0      0      0         0          3       0         1\n",
      "415       3    1      0      0         2          3       0         0\n",
      "416       3    1      0      0         2          3       0         0\n",
      "417       3    1      1      1         0          3       1         0\n",
      "\n",
      "[418 rows x 8 columns]\n",
      "418\n",
      "891\n",
      "[0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0\n",
      " 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
      " 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0\n",
      " 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0\n",
      " 0 0 1 0 1 0 0 1 0 0 1]\n",
      "[0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1\n",
      " 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0\n",
      " 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 0\n",
      " 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1\n",
      " 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0\n",
      " 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0\n",
      " 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1\n",
      " 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0\n",
      " 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0\n",
      " 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 1\n",
      " 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1\n",
      " 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0\n",
      " 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 0\n",
      " 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0\n",
      " 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1\n",
      " 0 1 0]\n",
      "418\n",
      "     PassengerId  Survived\n",
      "0            892         0\n",
      "1            893         0\n",
      "2            894         0\n",
      "3            895         0\n",
      "4            896         0\n",
      "..           ...       ...\n",
      "413         1305         0\n",
      "414         1306         1\n",
      "415         1307         0\n",
      "416         1308         0\n",
      "417         1309         1\n",
      "\n",
      "[418 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "test_data=pd.read_csv('/kaggle/input/titanic/test.csv')\n",
    "gender_sub=pd.read_csv('/kaggle/input/titanic/gender_submission.csv')\n",
    "print(test_data.head())\n",
    "\n",
    "\n",
    "test_data=pd.merge(test_data, gender_sub, on=['PassengerId'])\n",
    "\n",
    "test_data=test_data[['Pclass','Sex','SibSp','Parch','Embarked','Age','Fare','Survived']]\n",
    "print(train_data)\n",
    "\n",
    "test_data['Age']=test_data['Age'].fillna(test_data['Age'].mean())\n",
    "test_data['Fare']=test_data['Fare'].fillna(test_data['Fare'].mean())\n",
    "# train_data=train_data.drop_duplicates()\n",
    "\n",
    "test_data['Family']=0\n",
    "\n",
    "\n",
    "for x in range(len(test_data)):\n",
    "    sibsp=int(test_data['Parch'].iloc[x])\n",
    "  \n",
    "    if sibsp >0:\n",
    "        test_data['Family'].iloc[x]=1\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bins = [0,10,20,30,40,50,60,70,80,90,100]\n",
    "labels = [0,1,2,3,4,5,6,7,8,9]\n",
    "test_data['Age_Binned'] = pd.cut(test_data['Age'], bins=bins,labels=labels)\n",
    "bins = [0,32,64,128,256,512,1025]\n",
    "labels = [0,1,2,3,4,5]\n",
    "test_data['Fare_Binned'] = pd.cut(test_data['Fare'], bins=bins,labels=labels)\n",
    "\n",
    "\n",
    "\n",
    "enc_sex = pickle.load(open('./enc_sex.sav', 'rb'))\n",
    "test_data['Sex'] = enc_sex.transform(test_data['Sex'].tolist())\n",
    "\n",
    "enc_embarked = pickle.load(open('./enc_embarked.sav', 'rb'))\n",
    "test_data['Embarked'] = enc_embarked.transform(test_data['Embarked'].tolist())\n",
    "\n",
    "test_data=test_data[['Pclass','Sex','SibSp','Parch','Embarked','Age_Binned','Family','Survived']]\n",
    "print(test_data)\n",
    "\n",
    "np_input_data=test_data.iloc[:,0:len(train_data.columns)-1].values\n",
    "\n",
    "\n",
    "\n",
    "scaler = pickle.load(open('./scaling.sav', 'rb'))\n",
    "np_input_data_scaled = scaler.transform(np_input_data)\n",
    "\n",
    "\n",
    "\n",
    "poly = pickle.load(open('./polynomial.sav', 'rb'))\n",
    "np_input_data_scaled = poly.transform(np_input_data_scaled)\n",
    "\n",
    "\n",
    "pca = pickle.load(open('./pca.sav', 'rb'))\n",
    "np_input_data_scaled = pca.transform(np_input_data_scaled)\n",
    "\n",
    "np_output_data=train_data.iloc[:,len(train_data.columns)-1].values\n",
    "\n",
    "loaded_model = pickle.load(open('./gradien_boost.sav', 'rb'))\n",
    "result=loaded_model.predict(np_input_data_scaled)\n",
    "print(len(result))\n",
    "print(len(np_output_data))\n",
    "print(result)\n",
    "print(np_output_data)\n",
    "# print(\"Accuracy:\",metrics.accuracy_score(result, np_output_data))\n",
    "# print(\"Accuracy F1:\",metrics.f1_score(result, np_output_data, average='macro'))\n",
    "\n",
    "\n",
    "test_data=pd.read_csv('/kaggle/input/titanic/test.csv')\n",
    "print(len(test_data['PassengerId'].tolist()))\n",
    "result_df=pd.DataFrame()\n",
    "result_df['PassengerId']=test_data['PassengerId'].tolist()\n",
    "result_df['Survived']=result\n",
    "print(result_df)\n",
    "\n",
    "result_df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Pclass                                          Name     Sex  \\\n",
      "0          892       3                              Kelly, Mr. James    male   \n",
      "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
      "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
      "3          895       3                              Wirz, Mr. Albert    male   \n",
      "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
      "\n",
      "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
      "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
      "1  47.0      1      0   363272   7.0000   NaN        S  \n",
      "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
      "3  27.0      0      0   315154   8.6625   NaN        S  \n",
      "4  22.0      1      1  3101298  12.2875   NaN        S  \n",
      "     Pclass  Sex  SibSp  Parch  Embarked Age_Binned  Family  Survived\n",
      "0         3    1      1      0         2          2       0         0\n",
      "1         1    0      1      0         0          3       0         1\n",
      "2         3    0      0      0         2          2       0         1\n",
      "3         1    0      1      0         2          3       0         1\n",
      "4         3    1      0      0         2          3       0         0\n",
      "..      ...  ...    ...    ...       ...        ...     ...       ...\n",
      "886       2    1      0      0         2          2       0         0\n",
      "887       1    0      0      0         2          1       0         1\n",
      "888       3    0      1      2         2          2       1         0\n",
      "889       1    1      0      0         0          2       0         1\n",
      "890       3    1      0      0         1          3       0         0\n",
      "\n",
      "[891 rows x 8 columns]\n",
      "     Pclass  Sex  SibSp  Parch  Embarked Age_Binned  Family  Survived\n",
      "0         3    1      0      0         1          3       0         0\n",
      "1         3    0      1      0         2          4       0         1\n",
      "2         2    1      0      0         1          6       0         0\n",
      "3         3    1      0      0         2          2       0         0\n",
      "4         3    0      1      1         2          2       1         1\n",
      "..      ...  ...    ...    ...       ...        ...     ...       ...\n",
      "413       3    1      0      0         2          3       0         0\n",
      "414       1    0      0      0         0          3       0         1\n",
      "415       3    1      0      0         2          3       0         0\n",
      "416       3    1      0      0         2          3       0         0\n",
      "417       3    1      1      1         0          3       1         0\n",
      "\n",
      "[418 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived\n",
      "0            892         0\n",
      "1            893         0\n",
      "2            894         0\n",
      "3            895         0\n",
      "4            896         0\n",
      "..           ...       ...\n",
      "413         1305         0\n",
      "414         1306         1\n",
      "415         1307         0\n",
      "416         1308         0\n",
      "417         1309         1\n",
      "\n",
      "[418 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "test_data=pd.read_csv('/kaggle/input/titanic/test.csv')\n",
    "gender_sub=pd.read_csv('/kaggle/input/titanic/gender_submission.csv')\n",
    "print(test_data.head())\n",
    "\n",
    "\n",
    "test_data=pd.merge(test_data, gender_sub, on=['PassengerId'])\n",
    "\n",
    "test_data=test_data[['Pclass','Sex','SibSp','Parch','Embarked','Age','Fare','Survived']]\n",
    "print(train_data)\n",
    "\n",
    "test_data['Age']=test_data['Age'].fillna(test_data['Age'].mean())\n",
    "test_data['Fare']=test_data['Fare'].fillna(test_data['Fare'].mean())\n",
    "# train_data=train_data.drop_duplicates()\n",
    "\n",
    "test_data['Family']=0\n",
    "\n",
    "\n",
    "for x in range(len(test_data)):\n",
    "    sibsp=int(test_data['Parch'].iloc[x])\n",
    "  \n",
    "    if sibsp >0:\n",
    "        test_data['Family'].iloc[x]=1\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bins = [0,10,20,30,40,50,60,70,80,90,100]\n",
    "labels = [0,1,2,3,4,5,6,7,8,9]\n",
    "test_data['Age_Binned'] = pd.cut(test_data['Age'], bins=bins,labels=labels)\n",
    "bins = [0,32,64,128,256,512,1025]\n",
    "labels = [0,1,2,3,4,5]\n",
    "test_data['Fare_Binned'] = pd.cut(test_data['Fare'], bins=bins,labels=labels)\n",
    "\n",
    "\n",
    "\n",
    "enc_sex = pickle.load(open('./enc_sex.sav', 'rb'))\n",
    "test_data['Sex'] = enc_sex.transform(test_data['Sex'].tolist())\n",
    "\n",
    "enc_embarked = pickle.load(open('./enc_embarked.sav', 'rb'))\n",
    "test_data['Embarked'] = enc_embarked.transform(test_data['Embarked'].tolist())\n",
    "\n",
    "test_data=test_data[['Pclass','Sex','SibSp','Parch','Embarked','Age_Binned','Family','Survived']]\n",
    "print(test_data)\n",
    "\n",
    "np_input_data=test_data.iloc[:,0:len(train_data.columns)-1].values\n",
    "\n",
    "\n",
    "\n",
    "scaler = pickle.load(open('./scaling.sav', 'rb'))\n",
    "np_input_data_scaled = scaler.transform(np_input_data)\n",
    "\n",
    "\n",
    "\n",
    "poly = pickle.load(open('./polynomial.sav', 'rb'))\n",
    "np_input_data_scaled = poly.transform(np_input_data_scaled)\n",
    "\n",
    "\n",
    "pca = pickle.load(open('./pca.sav', 'rb'))\n",
    "np_input_data_scaled = pca.transform(np_input_data_scaled)\n",
    "\n",
    "np_output_data=train_data.iloc[:,len(train_data.columns)-1].values\n",
    "\n",
    "loaded_model = pickle.load(open('./gradien_boost.sav', 'rb'))\n",
    "gradien_boost_results=loaded_model.predict(np_input_data_scaled)\n",
    "\n",
    "loaded_model = pickle.load(open('./dt.sav', 'rb'))\n",
    "dt_results=loaded_model.predict(np_input_data_scaled)\n",
    "\n",
    "loaded_model = pickle.load(open('./xgb.sav', 'rb'))\n",
    "xgb_results=loaded_model.predict(np_input_data_scaled)\n",
    "\n",
    "loaded_model = pickle.load(open('./random_forest.sav', 'rb'))\n",
    "random_forest_results=loaded_model.predict(np_input_data_scaled)\n",
    "\n",
    "\n",
    "result_df=pd.DataFrame()\n",
    "\n",
    "result_df['gradient_boost']=gradien_boost_results\n",
    "result_df['dt']=dt_results\n",
    "\n",
    "result_df['xgb']=xgb_results\n",
    "\n",
    "result_df['random_forest']=random_forest_results\n",
    "\n",
    "\n",
    "result_sum=result_df.sum(axis=1)\n",
    "\n",
    "list_result=[]\n",
    "\n",
    "for x in range(len(result_sum)):\n",
    "    sum_=result_sum.iloc[x]\n",
    "    if sum_>2:\n",
    "        list_result.append(1)\n",
    "    else:\n",
    "        list_result.append(0)\n",
    "\n",
    "\n",
    "\n",
    "test_data=pd.read_csv('/kaggle/input/titanic/test.csv')\n",
    "\n",
    "result_df=pd.DataFrame()\n",
    "result_df['PassengerId']=test_data['PassengerId'].tolist()\n",
    "result_df['Survived']=list_result\n",
    "print(result_df)\n",
    "\n",
    "result_df.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
